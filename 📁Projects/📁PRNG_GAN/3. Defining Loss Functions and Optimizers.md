In this step:
1. Define the loss functions for the generator and discriminator
2. Set up optimizers to update their weights during training

---
## 3.1 Loss Functions
GAN training requires seperate loss functions for the generator and discriminator

1. **Discriminator Loss:** The discriminator must clarify
	1. **Real Data** as `1` (real)
	2. **Fake Data** from the generator as `0` (fake)
	> Using **Binary Cross-Entropy Loss** (BCE):
	> $$ L_D = -E[log(D(real))] - E[log(1-D(fake))] $$ 

2. **Generator Loss:** The generator's goal is to fool the discriminator into classifying its outputs as real:
	> $$ L_G = -E[log(D(fake))] $$

Loss Functions  #function 
```python
# Binary cross-entropy loss for discriminator and generator
cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)

def discriminator_loss(real_output, fake_output):
	"""
	Discriminator loss to classify real as 1 and fake as 0
	"""
	real_loss = cross_entropy(tf.ones_like(real_output), real_output)  # Real -> 1
	fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)  # Fake -> 0
	return real_loss + fake_loss

def generator_loss(fake_output):
	"""
	Generator loss to fool the discriminator (fake -> 1)
	"""
	return cross_entropy(tf.ones_like(fake_output), fake_output)  # Fake -> 1
```

---
## 3.2 Optimizers
Using **Adam Optimizer** for both models, which work well with GANs

Optimizers  #function 
```python
# Learning rates for generator and discriminator
generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)
discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)
```

---
## Testing Loss Functions
Testing loss functions with sample outputs

Testing code   #function 
```python
# Example real and fake outputs from discriminator
real_output = tf.constant([[0.8], [0.9]])   # Discriminator thinks real is likely real
fake_output = tf.constant([[0.3], [0.2]])   # Discriminator thinks fake is likely fake

# Calculate losses
d_loss = discriminator_loss(real_output, fake_output)
g_loss = generator_loss(fake_output)

print(f'Discriminator Loss: {d_loss.numpy()}')
print(f'Generator Loss: {g_loss.numpy()}')
```

### **Expected Output**

- **Discriminator Loss**: Higher loss if it struggles to distinguish real from fake.
- **Generator Loss**: Higher loss if the discriminator easily identifies fake outputs.