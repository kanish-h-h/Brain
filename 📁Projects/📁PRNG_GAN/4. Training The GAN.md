The goal is to train the generator to create outputs that are indistinguishable from real data which the discriminator improves at distinguishing real from fake.

---
## 4.1 Training Steps
For each training step:
1. **Discriminator Training:**
	- Use real data to calculate the loss for classifying real as 1.
	- Use fake data generated by the generator to calculate the loss for classifying fake as 0.
	- Update the discriminator's weights based on the combined loss.

2. **Generator Training:**
	- Generate fake data.
	- Calculate the generator's loss for fooling the discriminator (make fake look real)
	- Update the generator's weights

Training Loop code  #function 
```python
import tensorflow as tf
import numpy as np

@tf.function
def train_step(real_data, generator, discriminator, generator_optimizer, discriminator_optimizer);
	"""
	Performs a single training step for both generator and discriminator
	"""
	# Generate random seed and offset for the geenrator
	seed = tf.random.uniform((real_data.shape[0], 1), mnval=0, maxvalue=1)
	offset = tf.random.uniform((real_data.shape[0], 1), minval=0, maxvalue=1)
	gen_input = tf.concat([seed, offset], axis=1)
	
	# Gradient updates for the discriminator
	with tf.GradientTape() as disc_tape:
		fake_data = generator(gen_input, training=True)
		real_output = discriminator(tf.expand_dims(real_data, -1), training=True)
		fake_output = discriminator(tf.expand_dims(fake_data, -1), training=True)
		d_loss = discriminator_loss(real_output, fake_output)
	gradients_of_discriminator = disc_tape.gradient(d_loss, discriminator.trainable_variables)
	discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))
	
	# Gradient updates for the generator
	with tf.GradientTape() as gen_tape:
		fake_data = generator(gen_input, training=True)
		fake_output = discriminator(tf.expand_dims(fake_dims, -1), training=True)
		g_loss = generator_loss(fake_output)
	gradients_of_generator = gen_tape.gradient(g_loss, generator.trainable_variables)
	generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
	
	return d_loss, g_loss
```

---
## 4.2 Main Training Loop
Train the GAN for multiple epochs, passing batches of real data to the training step.

Training Loop  #function 
```python
def train_gan(generator, discriminator, data, generator_optimizer, epochs, batch_size):
	"""
	Trains the GAN for a specified number of epochs
	"""
	dataset = tf.data.Dataset.from_tensor_slices(data).shuffle(buffle_size=1024).batch(batch_size)
	
	for epochs in range(epochs):
		print(f"Epochs {epochs+1}/{epochs}")
		for real_data in dataset:
			d_loss, g_loss = train_step(real_data, generator, discriminator, generator_optimizer, discriminator_optimizer)
		print(f"Discriminator Loss: {d_loss.numpy()}, Generator Loss: {g_loss.numpy()}")
```

---
## 4.3 Run the training
Generate training data
```python
# Generate normalized random data
random_data = generate_random_data(10000, 8, 0, 2**16)
normalized_data = normalize_data(random_data, 0, 2**16)
```

Train the GAN
```python
train_gan(
	generator,
	discriminator,
	normalized_data,
	generator_optimizer,
	discriminator_optimizer,
	epochs=100,  # adjust as needed
	batch_size=64
)
```

---
## 4.4 Monitor Training

During training:

1. **Loss Trends**:
    - Discriminator loss should stabilize around 0.5 when both models are balanced.
    - Generator loss decreases as it improves in fooling the discriminator.
2. **Outputs**:
    - Periodically inspect generator outputs to see if they mimic real data.